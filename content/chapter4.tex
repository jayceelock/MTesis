\chapter{Pose Error Estimation}
\label{chap4}

\section{Introduction}

The main objective of this project is to determine the pose estimation accuracy of a quadcopter's on-board sensor suite. The pose estimation must be performed in the outdoors to provide the quadcopter access to its GPS readings, which is important data for an outdoor quadcopter since it provides absolute position data which can be used to offset the sensor drift introduced by integrating other sensor data. To this end, a computer vision pose measurement system (CVS) which extracts six-dimensional pose data from a calibration object in the system's view, was designed and implemented. However, before the CVS could be used to make pose measurements, its measurement accuracy was first determined. 

In Chapter~\ref{chap3} it was found that the pose measurement error of the CVS displays high interdimensional dependence, as demonstrated by the covariance matrix of the CVS's measurement error data in Equation~\ref{eq:covariance-matrix} and the contour plots in Figure~\ref{fig:err-contour}. This means that the CVS's pose measurements and corresponding measurement errors are constantly varying according to the relative pose between the calibration board and the CVS's camera. Also, despite using the RANSAC algorithm to filter out outlier data points, the CVS's measurement data is still fairly noisy, particularly in the orientation dimensions.

In the context of this project it is important to know what the accuracy of a pose measurement sample is. This makes it necessary to be able to determine the measurement error of any arbitrary pose measurement sample recorded by the CVS.\@ It was decided that a machine learning method will be employed to accomplish this. Machine learning is where a computer model is trained to recognise patterns within a set of input data and output information on the patterns which are of interest to the model's designer. 

This chapter sets out to explain the process behind making a machine learning model that can estimate, with reasonable accuracy, what the CVS's pose measurement error for any arbitrary input measurement vector would be. This chapter has been presented at the SolarPACES 2015 conference in Cape Town and a conference proceedings article has been accepted~\citep{lock2015}. 

The chapter starts with some background information on model selection, design and training. The validation phase of the trained model is then discussed and the accuracy of the final model is presented, followed by a brief discussion on the resulting model. 

\section{Model Design}

This section discusses the design aspects of the machine learning model used for this project. It begins by giving some background information on the processes involved in designing such a model, including model type selection, training and validation. These aspects are discussed in more detail here.  

\subsection{Background}

Designing a machine learning model can be performed in three basic steps. These steps are:

\begin{enumerate}
  \item Select machine learning model type.
  \item Train the model.
  \item Validate the trained model's accuracy. 
\end{enumerate}

First, the model type is selected. As stated previously, the CVS's pose measurement data is fairly complex, interdimensionally dependant and high dimensional. A neural network was therefore selected as a machine learning model basis. It has previously been shown by~\cite{tu1996advantages} that neural networks excel at handling complicated data and detecting and extracting complex, non-linear relationships within the input data. Furthermore, the radial basis function neural network (RBFNN) was selected as the network topology. RBFNN's provide superior results when compared to traditional feed-forward networks when noisy data sets are used, as shown by~\cite{xie2011comparison}. The data is not time dependant, making the recurrent neural network topology unnecessarily complex and costly for this application. An RBFNN is described by the relation given by Equation~\ref{eq:chap2-rbf} and is repeated again in Equation~\ref{eq:chap4-rbf}.

\begin{equation}
  \label{eq:chap4-rbf}
  f(\bm{x}_i) = \sum\limits_{j = 1}^{M}\lambda_j \phi(|| \bm{x}_i - \bm{x}_j ||)
\end{equation}
Here, $\phi$ is a function of the Euclidean distance between the radial basis centres, $\bm{x}_j$, and the sample points, $\bm{x}_i$.

Next, model training takes place. Here the network is trained to take an input and adjust is internal weights in such a way that the input data is best translated to the desired output by using a supervised training scheme, such as the backpropagation procedure. In this case, the network input would be the six-dimensional pose vector produced by the CVS and the network output is the corresponding measurement error of the input pose vectors.  

Finally, to test and validate the accuracy of the trained model, it is tested with another set of input pose data of which the measurement error is already known. This data set can then be used to select and refine the training parameters of the network model. 

The design process is iterative, where steps 2 and 3, and perhaps 1, can be repeated until a network which outputs satisfactory results is produced. 

\subsection{Network Training}

Training a machine learning model is perhaps the most crucial aspect of the model design phase. Here, the nodes of the RBFNN are initialised and assigned a weighting factor. The weightings are then adjusted and optimised through a supervised training method called backprogagation, demonstrated in Figure~\ref{fig:chap4-backprogagation}. The number of radial bases can also be optimised to produce the best results, but in this case it is preselected. 

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/chapter4/backpropagation}
  \caption[A neural network implementing the backpropagation procedure.]{A figure of a neural network implementing the backpropagation procedure. Adapted from~\cite{ann-wiki-pic}.}
\label{fig:chap4-backprogagation}
\end{figure}

Backpropagation works as follows. The hidden nodes are given an initial weighting factor. The input training data is then fed to the network and the network's output is then compared to the training set's output data. Based on this difference, the hidden nodes' weightings are adjusted to minimise the difference. Then, the input data is fed to the adjusted network again and its new output is compared to the output training data, where the weights are adjusted again. This process is repeated until the mean square error (MSE) of the output's deviation from the training data falls below a set threshold or converges to zero. 

Factors that will affect the network's accuracy are the number of nodes, or radial bases, in the hidden layer as well as the strictness of fit that the designer selects. The number of hidden nodes is a measure of the complexity of the data the network process and also reflects the its own complexity. However, when too many nodes are initialised, there is a significant risk that the model will capture and attempt to model the noise and outlier data in a data set, instead of only characterising the underlying relationships between the input and output. This phenomenon is known as overfitting and can be observed when a model with a very small training error is exposed to new input data and the error is a few orders of magnitude larger than for the training data. Therefore, the model validation phase is also an important aspect of model training to ensure that overfitting does not take place. 

The strictness of fit measure determines to what extent the network will attempt to fit itself to the training data. Selecting the right parameter is important, since neglecting to replicate the training data adequately defeats the purpose of training the network to fit the data in the first place. However, if this parameter is too strict, the network will attempt to model the outlier and noisy data points as well, which is undesirable. Therefore, a fine balance needs to be found for the strictness of fit and the number of hidden nodes to produce a satisfactory network.  

To train the network of this project, two sets of data was used; one set for training and another for validation. Both of these sets were taken from the data generated during the Vicon measurement test where the measurement accuracy of the CVS was determined. It includes both the pose data from the CVS and its corresponding measurement error. It was decided to use 300 training samples, giving 50 samples per dimension. The training data was selected to be uniformly distributed to place emphasis on the entire measurement spectrum. The uniformity of the data in each dimension was verified with the Chi-square ($\chi^2$) goodness-of-fit test, with the hypothesis being that the set is uniformly distributed. As a rule of thumb, if the P-value for the $\chi^2$ test falls below 5 \%, then the hypothesis is statistically insignificant and must therefore be rejected. The training data was selected in such a way that the P-value did not fall below 45 \% for the data in each dimension. Furthermore, since the pose vector contains different measurement units (degrees and metres), the input and output training data sets were normalised to a range of $[-1, 1]$. All subsequent inputs to and outputs from the network must be normalised and denormalised with the same values that the training data was normalised with.  

Matlab's\footnote{Matlab v8.4.0.150421 (R2014)} Neural Network toolbox\footnote{Neural Network Toolbox v8.2.1} was used to train the RBFNN and get the output for subsequent input data. The function prototype is given by 

\begin{center}
  \verb|rbf = newrb(P,T,goal,spread,MN,DF)|.
\end{center}

Here, $P$ and $T$ are the input and output training data matrices. The \emph{goal} parameter is the error threshold for the training data and \emph{spread} is the strictness of fit parameter, while $\mathit{MN}$ and $\mathit{DF}$ define the maximum number of hidden nodes and dictate the amount of nodes to increase between each training iteration. Adjusting the \emph{spread} and $\mathit{MN}$ parameters are the primary ways of manipulating the network to produce more accurate outputs. 

To accommodate the inherent differences between the position and orientation dimensions, two RBFNNs were trained. It was found that using two different networks produces results more accurate than when a single network was used to model the data. 

\subsection{Model Validation}

A model validation procedure was used to ensure that the RBFNN was properly trained and that overfitting did not occur during training. The validation set comes from the same Vicon test that the training data was selected from. Another 300 randomly selected  pose measurement samples were used for the validation data set. Given that the training data is uniformly distributed, the validation set should fall within the training data limits. Figure~\ref{fig:chap4-scatter-tr-v} displays the scatter plots for the position and orientation vectors for the training and validation sets and shows that they indeed do overlap to a large degree. 

\begin{figure*}
  \centering
  \begin{subfigure}{\textwidth}
    \begin{subfigure}{0.48\textwidth}
      \includegraphics[clip, trim = 80 0 80 0, width=\textwidth]{figures/chapter4/trv_xy}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
      \includegraphics[clip, trim = 80 0 80 0, width=\textwidth]{figures/chapter4/trv_xz}
    \end{subfigure}
    \caption{}
  \end{subfigure}
  ~
  \begin{subfigure}{\textwidth}
    \begin{subfigure}{0.48\textwidth}
      \includegraphics[clip, trim = 80 0 80 0, width=\textwidth]{figures/chapter4/trv_rollpitch}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
      \includegraphics[clip, trim = 80 0 80 0, width=\textwidth]{figures/chapter4/trv_rollyaw}
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \caption[Scatter plots of the training and validation data. ]{Normalised scatter plots of the translation and rotation data points for the training and validation data sets. Note that the two data sets overlap. This is done on purpose to ensure that the RBFNN does not extrapolate values when the validation set is used as input. }
  \label{fig:chap4-scatter-tr-v}
\end{figure*}

The measurement error was determined by feeding the validation input data into the trained networks and comparing their outputs with the validation error data. The MSE between the networks' output and validation data was used as a measure of accuracy. The validation set was used to tune the \emph{spread} and $\mathit{MN}$ training parameters, where the best combination would produce the smallest training and validation MSEs. 

\section{Results}

With the training and validation data sets selected, the network design could begin. Different combinations of training parameters for both networks were tested. It was found that the \emph{spread} parameter had the largest influence over the MSE during training and validation. This may be because the input data (the orientation in particular) changes relatively quickly and contains a fair amount of noise. It was found that forcing the RBFNN to go through all the points caused the MSE to rise in both networks when they were tested with the validation set, indicating that the networks were being overfitted. It was therefore decided to keep the \emph{spread} parameter relatively small to allow the RBFNNs to ignore what they classify as outlier data and rather describe the general trend in the data. 

Furthermore, changing the maximum number of hidden nodes also influenced the accuracy of the networks. Leaving the $\mathit{MN}$ parameter to its default value will let the maximum number of hidden nodes go up to the number of training samples (300 nodes in this case), making the networks overly complex and slow. It was found that the networks are more accurate with both the training and validation data sets when the hidden nodes number only about 10 \% of the number of training samples. 

UPDATE
A training parameter combination that was found to work well for the translation network MEMEME was a network with a \emph{spread} of 8\e{-1} and 8 hidden nodes. This combination gives a normalised MSE of approximately 1.9\e{-2} with the validation data set, and 5.1\e{-3} with the training set, whereas the default values of 1 and 300 give an MSE of 3.14\e{4} and 7.1\e{-5} for the validation and training sets respectively. The results with the 300 hidden nodes demonstrates the effect of overfitting well: the training set produces very small errors, but gives a very large deviation when used with a validation data set.    

Figure~\ref{fig:chap4-rbf-train} shows results of the RBFNN when tested with the training data set. It can be seen that the network's estimate very closely resembles the training set's error, indicating that the network is well-trained. 

\begin{figure*}
  \begin{subfigure}{0.48\textwidth}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_tr_x}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_tr_y}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_tr_z}
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_tr_roll}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_tr_pitch}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_tr_yaw}
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \caption[The output of the RBFNN when used with the training set input.]{Plots of the output of the RBFNN for the different dimensions when used with the training data set.}
  \label{fig:chap4-rbf-train}
\end{figure*}

Figure~\ref{fig:chap4-rbf-valid} shows the network's output when tested with the validation data set. Here it can be seen that the RBFNN's error estimate deviates somewhat from the true error, however, it follows the general trend in the data and ignores the noisy outlier data spikes. Furthermore, it generally gives an overly conservative estimate, often overestimating the error for a pose vector. This is not ideal, but is safer than underestimating the error. In general, the position dimensions are fairly accurate. However, with the exception of the $pitch$ dimension, the orientation dimensions show significant deviations from the true error, at times deviating up to $\ang{80}$ from the true error. It is suspected that the high amount of noise within the data is to blame for the bad performance in the orientation dimensions. 

\begin{figure*}
  \begin{subfigure}{0.48\textwidth}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_v_x}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_v_y}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_v_z}
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_v_roll}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_v_pitch}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
      \includegraphics[clip, trim = 100 0 100 0, width=\textwidth]{figures/chapter4/scatter_v_yaw}
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \caption[The output of the RBFNN with the validation set input.]{Plots of the output for the different dimensions of the RBFNN when used with the validation data set.}
  \label{fig:chap4-rbf-valid}
\end{figure*}

\section{Conclusion}

In this chapter, a neural network was designed to model and predict the complex and dimensionally dependant pose measurement error of the CVS.\@ A radial basis function neural network was selected to do this: it takes a six-dimensional pose measurement vector from the CVS and outputs the corresponding measurement error for the input sample. The network was trained and validated by two different data sets from the same measurement test and produces a mean square error of 1.9\e{-2} for the validation set, indicating a good model fit for the data. 

The trained network is now ready to predict the measurement error for the CVS with pose measurement data generated by a quadcopter in flight. This data can then be used to determine the pose estimation accuracy of a drone in flight. 
